# -*- coding: utf-8 -*-
"""LSTM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1INSsDD4QlmPbUjD7wxnnXkwsM42iFImG
"""

import zipfile
import os

# Path to the ZIP file
zip_file_path = '/content/TrainingData.zip'

# Destination folder where files will be extracted
extract_to_folder = '/content/'

# Ensure the destination folder exists
os.makedirs(extract_to_folder, exist_ok=True)

# Open the ZIP file
with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
    # Extract all the contents to the destination folder
    zip_ref.extractall(extract_to_folder)

print(f"Files extracted to '{extract_to_folder}'")

from google.colab import drive
drive.mount('/content/drive')

"""### Import libraries"""

# Import necessary libraries
import os

# Import necessary libraries for data processing
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
import numpy as np

# Import necessary libraries for model
from keras.callbacks import Callback
from keras.models import Sequential, load_model
from keras.layers import LSTM, Dense
from keras.metrics import Precision, Recall, F1Score, AUC

# Import necessary libraries for evaluation
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

"""## Directories"""

base_dir = '/content/drive/MyDrive/TrainingData'
Save_model_dir = 'LSTM_model'

"""# Data Preparation

### Load and merge data
"""

# List of stocks and splits
periods = ['1']
stocks = ['A', 'B', 'C', 'D', 'E']
splits = ['0', '1', '2', '3', '4','5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15']

# Load the data from a CSV file
market_data_list = []
trade_data_list = []

# loop over the stocks and load the data for each period
for period in periods:

  # check for nested period dir
  period_path = os.path.join(base_dir, f'Period{period}')
  nested_period_path = os.path.join(period_path, f'Period{period}')

  # Handle nested data
  if os.path.exists(nested_period_path):
      period_dir_to_use = nested_period_path
  else:
      period_dir_to_use = period_path


  for stock in stocks:

      trade_data_dir = f'{period_dir_to_use}/{stock}/trade_data__{stock}.csv'
      if os.path.exists(trade_data_dir):
          trade_data = pd.read_csv(trade_data_dir)
          trade_data['stock'] = stock
          trade_data_list.append(trade_data)
      else:
          print(f"File {trade_data_dir} does not exist")
          continue

      for split in splits:
          market_data_dir = f'{period_dir_to_use}/{stock}/market_data_{stock}_{split}.csv'

          if os.path.exists(market_data_dir):
              market_data = pd.read_csv(market_data_dir)

              # Add stock identifier
              market_data['stock'] = stock
              market_data_list.append(market_data)
          else:
              print(f"File {market_data_dir} does not exist")


# Combine market and trade data for the period
period_market_data = pd.concat(market_data_list, ignore_index=True)
period_trade_data = pd.concat(trade_data_list, ignore_index=True)

# Merge the data on the nearest timestamp
period_market_data['timestamp'] = pd.to_datetime(period_market_data['timestamp'], errors='coerce')
period_trade_data['timestamp'] = pd.to_datetime(period_trade_data['timestamp'], errors='coerce')

# Drop any rows where timestamp is null
period_market_data.dropna(subset=['timestamp'], inplace=True)
period_trade_data.dropna(subset=['timestamp'], inplace=True)


# Sort by stock, then timestamp, and reset index
period_market_data = (
    period_market_data
    .sort_values(['stock', 'timestamp'], ascending=[True, True])
    .reset_index(drop=True)
)
period_trade_data = (
    period_trade_data
    .sort_values(['stock', 'timestamp'], ascending=[True, True])
    .reset_index(drop=True)
)

# **Ensure 'timestamp' is sorted within each 'stock' group in period_market_data**
period_market_data = period_market_data.groupby('stock').apply(lambda x: x.sort_values('timestamp')).reset_index(drop=True)
period_trade_data = period_trade_data.groupby('stock').apply(lambda x: x.sort_values('timestamp')).reset_index(drop=True)

# Optionally verify monotonic_increasing in each group
for s in period_market_data['stock'].unique():
    subset = period_market_data[period_market_data['stock'] == s]
    if not subset['timestamp'].is_monotonic_increasing:
        print(f"period_market_data has out-of-order timestamps for stock {s}")

for s in period_trade_data['stock'].unique():
    subset = period_trade_data[period_trade_data['stock'] == s]
    if not subset['timestamp'].is_monotonic_increasing:
        print(f"period_trade_data has out-of-order timestamps for stock {s}")

# Merge market and trade data
merged_data = pd.merge_asof(
    period_market_data,
    period_trade_data,
    on='timestamp',
    by='stock',
    direction='nearest',
    tolerance=pd.Timedelta(milliseconds=10),
    allow_exact_matches=False
)

# Save merged_data as a CSV
merged_data.to_csv('merged_data.csv', index=False)

"""### Feature Engineering"""

# define significant change as a percentage change of more than 1%
threshold = 0.01

# Calculate the spread between the market and trade data
merged_data['price_change'] = merged_data['price'].pct_change()

# calculate and label significant changes
merged_data['significant_change'] = (merged_data['price_change'].abs() > threshold).astype(int)

# Add a moving average of the spread
merged_data['spread'] = merged_data['askPrice'] - merged_data['bidPrice']
merged_data['bidPrice_ma'] = merged_data.groupby('stock')['bidPrice'].rolling(window=5).mean().reset_index(0, drop=True)
merged_data['askPrice_ma'] = merged_data.groupby('stock')['askPrice'].rolling(window=5).mean().reset_index(0, drop=True)

# Fill any remaining NANS by backfilling
merged_data.fillna(method='bfill', inplace=True)
merged_data.fillna(method='ffill', inplace=True)

"""### Define the feature set and target set"""

# Define the feature and it's fitted scaller
features = [
    'bidVolume',
    'bidPrice',
    'askVolume',
    'askPrice',
    'spread',
    'bidPrice_ma',
    'askPrice_ma'
    ]

scaler = MinMaxScaler()
merged_data[features] = scaler.fit_transform(merged_data[features])

"""### Preparing input"""

# define the timesteps
timesteps = 10

# Prepare input (x) and output (y)
x = []
y = []

for stock in stocks:
    stock_data = merged_data[merged_data['stock'] == stock]
    stocks_features = stock_data[features].values
    stocks_target = stock_data[['significant_change']].values

    for i in range(len(stock_data) - timesteps):
        x.append(stocks_features[i:i + timesteps])
        y.append(stocks_target[i + timesteps])

x = np.array(x)
y = np.array(y)

"""### Split data"""

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

"""# Implementation of Model

### Define Model class
"""

# reshape the input data
input_shape = (x_train.shape[1], x_train.shape[2])

# define the LSTM model
model = Sequential()
model.add(LSTM(100, input_shape=input_shape, return_sequences=True))
model.add(Dense(1, activation='sigmoid'))

"""### Compile model"""

# compile the model for training
model.compile(
    loss='binary_crossentropy',
    optimizer='adam',
    metrics=['accuracy']
)

"""### Train and validate model"""

# # train the model
# model.fit(x_train, y_train, epochs=20, batch_size=32, validation_data=(x_test, y_test))


# Custom callback to save and print metrics at each epoch
class EpochLogger(Callback):
    def __init__(self):
        super().__init__()
        self.epochs = []

    def on_epoch_end(self, epoch, logs=None):
        logs = logs or {}
        # Print the metrics for the current epoch
        print(f"Epoch {epoch + 1}: {logs}")

        # Save metrics in a list for later use
        self.epochs.append({"epoch": epoch + 1, **logs})

# Instantiate the custom callback
epoch_logger = EpochLogger()

# Train the model and use the custom callback
history = model.fit(
    x_train,
    y_train,
    epochs=20,
    batch_size=32,
    validation_data=(x_test, y_test),
    callbacks=[epoch_logger]
)

# Save the epoch logs to a CSV file
epoch_logs_df = pd.DataFrame(epoch_logger.epochs)
epoch_logs_df.to_csv('epoch_logs.csv', index=False)

# Print the saved dataframe
print(epoch_logs_df)

"""### Test model"""

# Predict on the scaled test data
y_pred_scaled = model.predict(x_test)

# Apply inverse transformation to scale back to the original range and inverse transform y_test
y_pred_binary = (y_pred_scaled > 0.5).astype(int)

print("Predicted significant changes: (Binary)", y_pred_binary.ravel()[:10])
print("Actual significant changes: (Binary)", y_test.ravel()[:10])

"""## Other resources code

### Load model
"""

loaded_model = load_model(Save_model_dir)

"""### Save model"""

# save in tensoflow model
model.save(Save_model_dir)
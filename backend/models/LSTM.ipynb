{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Path to the ZIP file\n",
        "zip_file_path = '/content/TrainingData.zip'\n",
        "\n",
        "# Destination folder where files will be extracted\n",
        "extract_to_folder = '/content/'\n",
        "\n",
        "# Ensure the destination folder exists\n",
        "os.makedirs(extract_to_folder, exist_ok=True)\n",
        "\n",
        "# Open the ZIP file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    # Extract all the contents to the destination folder\n",
        "    zip_ref.extractall(extract_to_folder)\n",
        "\n",
        "print(f\"Files extracted to '{extract_to_folder}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRoz3CA7jNzF",
        "outputId": "8f9ea77a-ae9d-442c-ed0d-0c2ae93e456d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files extracted to '/content/'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xWrviVqQjqWI",
        "outputId": "528a8f16-52ed-4ad2-b236-d97feab98d0b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJW4DJj7OMYy"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "ucfgs04lOMYz"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "\n",
        "# Import necessary libraries for data processing\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Import necessary libraries for model\n",
        "from keras.callbacks import Callback\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import LSTM, Dense\n",
        "from keras.metrics import Precision, Recall, F1Score, AUC\n",
        "\n",
        "# Import necessary libraries for evaluation\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNvxW0SbOZyq"
      },
      "source": [
        "## Directories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "pLjNDLO0OgEd"
      },
      "outputs": [],
      "source": [
        "base_dir = '/content/drive/MyDrive/TrainingData'\n",
        "Save_model_dir = 'LSTM_model'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJ0EbNkkOMY0"
      },
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blmYvhkoOMY0"
      },
      "source": [
        "### Load and merge data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ptCZnn4OMY0",
        "outputId": "c7bfbf43-f4ed-4969-830c-89ce78c910e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File /content/drive/MyDrive/TrainingData/Period1/A/market_data_A_2.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/A/market_data_A_3.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/A/market_data_A_4.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/A/market_data_A_5.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/A/market_data_A_6.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/A/market_data_A_7.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/A/market_data_A_8.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/A/market_data_A_9.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/A/market_data_A_10.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/A/market_data_A_11.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/A/market_data_A_12.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/A/market_data_A_13.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/A/market_data_A_14.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/A/market_data_A_15.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/B/market_data_B_1.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/B/market_data_B_2.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/B/market_data_B_3.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/B/market_data_B_4.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/B/market_data_B_5.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/B/market_data_B_6.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/B/market_data_B_7.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/B/market_data_B_8.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/B/market_data_B_9.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/B/market_data_B_10.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/B/market_data_B_11.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/B/market_data_B_12.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/B/market_data_B_13.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/B/market_data_B_14.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/B/market_data_B_15.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/C/market_data_C_5.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/C/market_data_C_6.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/C/market_data_C_7.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/C/market_data_C_8.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/C/market_data_C_9.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/C/market_data_C_10.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/C/market_data_C_11.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/C/market_data_C_12.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/C/market_data_C_13.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/C/market_data_C_14.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/C/market_data_C_15.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/D/market_data_D_2.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/D/market_data_D_3.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/D/market_data_D_4.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/D/market_data_D_5.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/D/market_data_D_6.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/D/market_data_D_7.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/D/market_data_D_8.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/D/market_data_D_9.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/D/market_data_D_10.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/D/market_data_D_11.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/D/market_data_D_12.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/D/market_data_D_13.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/D/market_data_D_14.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/D/market_data_D_15.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/E/market_data_E_7.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/E/market_data_E_8.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/E/market_data_E_9.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/E/market_data_E_10.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/E/market_data_E_11.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/E/market_data_E_12.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/E/market_data_E_13.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/E/market_data_E_14.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/E/market_data_E_15.csv does not exist\n"
          ]
        }
      ],
      "source": [
        "# List of stocks and splits\n",
        "periods = ['1']\n",
        "stocks = ['A', 'B', 'C', 'D', 'E']\n",
        "splits = ['0', '1', '2', '3', '4','5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15']\n",
        "\n",
        "# Load the data from a CSV file\n",
        "market_data_list = []\n",
        "trade_data_list = []\n",
        "\n",
        "# loop over the stocks and load the data for each period\n",
        "for period in periods:\n",
        "\n",
        "  # check for nested period dir\n",
        "  period_path = os.path.join(base_dir, f'Period{period}')\n",
        "  nested_period_path = os.path.join(period_path, f'Period{period}')\n",
        "\n",
        "  # Handle nested data\n",
        "  if os.path.exists(nested_period_path):\n",
        "      period_dir_to_use = nested_period_path\n",
        "  else:\n",
        "      period_dir_to_use = period_path\n",
        "\n",
        "\n",
        "  for stock in stocks:\n",
        "\n",
        "      trade_data_dir = f'{period_dir_to_use}/{stock}/trade_data__{stock}.csv'\n",
        "      if os.path.exists(trade_data_dir):\n",
        "          trade_data = pd.read_csv(trade_data_dir)\n",
        "          trade_data['stock'] = stock\n",
        "          trade_data_list.append(trade_data)\n",
        "      else:\n",
        "          print(f\"File {trade_data_dir} does not exist\")\n",
        "          continue\n",
        "\n",
        "      for split in splits:\n",
        "          market_data_dir = f'{period_dir_to_use}/{stock}/market_data_{stock}_{split}.csv'\n",
        "\n",
        "          if os.path.exists(market_data_dir):\n",
        "              market_data = pd.read_csv(market_data_dir)\n",
        "\n",
        "              # Add stock identifier\n",
        "              market_data['stock'] = stock\n",
        "              market_data_list.append(market_data)\n",
        "          else:\n",
        "              print(f\"File {market_data_dir} does not exist\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Columns in market_data:\", market_data.columns)\n",
        "print(\"Columns in trade_data:\", trade_data.columns)\n",
        "\n",
        "# Convert timestamps to datetime (if they aren't already)\n",
        "market_data['timestamp'] = pd.to_datetime(market_data['timestamp'])\n",
        "trade_data['timestamp'] = pd.to_datetime(trade_data['timestamp'])\n",
        "\n",
        "market_data = market_data.sort_values(by='timestamp')\n",
        "trade_data = trade_data.sort_values(by='timestamp')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OeNlEBL7HBAv",
        "outputId": "73242507-2728-4bc3-b8c8-193eb9af05bb"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns in market_data: Index(['bidVolume', 'bidPrice', 'askVolume', 'askPrice', 'timestamp', 'stock'], dtype='object')\n",
            "Columns in trade_data: Index(['price', 'volume', 'timestamp', 'stock'], dtype='object')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-31-6222e879ba96>:5: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  market_data['timestamp'] = pd.to_datetime(market_data['timestamp'])\n",
            "<ipython-input-31-6222e879ba96>:6: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  trade_data['timestamp'] = pd.to_datetime(trade_data['timestamp'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "merged_data = pd.merge_asof(\n",
        "    market_data.sort_values(by=[\"stock\", \"timestamp\"]),\n",
        "    trade_data[['stock','timestamp','price','volume']].sort_values(by=[\"stock\", \"timestamp\"]),\n",
        "    on='timestamp',\n",
        "    by='stock',            # match on the same stock symbol\n",
        "    direction='nearest',\n",
        "    tolerance=pd.Timedelta(seconds=1)\n",
        ")"
      ],
      "metadata": {
        "id": "zg-IX-sPHN2C"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6lkTQjKOMY0"
      },
      "source": [
        "### Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "K0_LdDQah51j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d9a1084-0855-4e09-8ae0-67254edc90b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-33-e51a599c65eb>:5: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
            "  merged_data['price_change'] = merged_data['price'].pct_change()\n"
          ]
        }
      ],
      "source": [
        "# define significant change as a percentage change of more than 1%\n",
        "threshold = 0.01\n",
        "\n",
        "# Calculate the spread between the market and trade data\n",
        "merged_data['price_change'] = merged_data['price'].pct_change()\n",
        "\n",
        "# calculate and label significant changes\n",
        "merged_data['significant_change'] = (merged_data['price_change'].abs() > threshold).astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6BKXhtTGOMY0",
        "outputId": "5ffc2cb0-cc85-4734-ac53-63978c8408bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-34-3e3a40589b58>:7: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  merged_data.fillna(method='bfill', inplace=True)\n",
            "<ipython-input-34-3e3a40589b58>:8: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  merged_data.fillna(method='ffill', inplace=True)\n"
          ]
        }
      ],
      "source": [
        "# Add a moving average of the spread\n",
        "merged_data['spread'] = merged_data['askPrice'] - merged_data['bidPrice']\n",
        "merged_data['bidPrice_ma'] = merged_data.groupby('stock')['bidPrice'].rolling(window=5).mean().reset_index(0, drop=True)\n",
        "merged_data['askPrice_ma'] = merged_data.groupby('stock')['askPrice'].rolling(window=5).mean().reset_index(0, drop=True)\n",
        "\n",
        "# Fill any remaining NANS by backfilling\n",
        "merged_data.fillna(method='bfill', inplace=True)\n",
        "merged_data.fillna(method='ffill', inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajLBxHIuOMY1"
      },
      "source": [
        "### Define the feature set and target set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "xRqfoZDjOMY1"
      },
      "outputs": [],
      "source": [
        "# Define the feature and it's fitted scaller\n",
        "features = [\n",
        "    'bidVolume',\n",
        "    'bidPrice',\n",
        "    'askVolume',\n",
        "    'askPrice',\n",
        "    'spread',\n",
        "    'bidPrice_ma',\n",
        "    'askPrice_ma'\n",
        "    ]\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "merged_data[features] = scaler.fit_transform(merged_data[features])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdWioYnnzgwh"
      },
      "source": [
        "### Preparing input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "2sO25n-Gh51k"
      },
      "outputs": [],
      "source": [
        "# define the timesteps\n",
        "timesteps = 10\n",
        "\n",
        "# Prepare input (x) and output (y)\n",
        "x = []\n",
        "y = []\n",
        "\n",
        "for stock in stocks:\n",
        "    stock_data = merged_data[merged_data['stock'] == stock]\n",
        "    stocks_features = stock_data[features].values\n",
        "    stocks_target = stock_data[['significant_change']].values\n",
        "\n",
        "    for i in range(len(stock_data) - timesteps):\n",
        "        x.append(stocks_features[i:i + timesteps])\n",
        "        y.append(stocks_target[i + timesteps])\n",
        "\n",
        "x = np.array(x)\n",
        "y = np.array(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eFiRRfOOMY1"
      },
      "source": [
        "### Split data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "LgANT82iOMY1"
      },
      "outputs": [],
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLks3o1jOMY1"
      },
      "source": [
        "# Implementation of Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vfDb4qDOMY2"
      },
      "source": [
        "### Define Model class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWGBvb4zOMY2",
        "outputId": "93bc95f1-7403-4850-f461-952a6b08b216"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        }
      ],
      "source": [
        "# reshape the input data\n",
        "input_shape = (x_train.shape[1], x_train.shape[2])\n",
        "\n",
        "# define the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(100, input_shape=input_shape, return_sequences=False))\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUCwJCG9OMY2"
      },
      "source": [
        "### Compile model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "JcCbAQQHOMY2"
      },
      "outputs": [],
      "source": [
        "# compile the model for training\n",
        "model.compile(\n",
        "    loss='binary_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQo_W946OMY2"
      },
      "source": [
        "### Train and validate model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nqs219boOMY2",
        "outputId": "0e986798-9e06-4395-98d7-40f58f1c37e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m185/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9231 - loss: 0.1541Epoch 1: {'accuracy': 0.9848987460136414, 'loss': 0.040466438978910446, 'val_accuracy': 1.0, 'val_loss': 3.9413389458786696e-05}\n",
            "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 16ms/step - accuracy: 0.9247 - loss: 0.1511 - val_accuracy: 1.0000 - val_loss: 3.9413e-05\n",
            "Epoch 2/20\n",
            "\u001b[1m182/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 3.2760e-05Epoch 2: {'accuracy': 1.0, 'loss': 2.739078990998678e-05, 'val_accuracy': 1.0, 'val_loss': 1.9205119315302e-05}\n",
            "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 3.2538e-05 - val_accuracy: 1.0000 - val_loss: 1.9205e-05\n",
            "Epoch 3/20\n",
            "\u001b[1m184/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 1.6871e-05Epoch 3: {'accuracy': 1.0, 'loss': 1.4972746612329502e-05, 'val_accuracy': 1.0, 'val_loss': 1.178236743726302e-05}\n",
            "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 1.6812e-05 - val_accuracy: 1.0000 - val_loss: 1.1782e-05\n",
            "Epoch 4/20\n",
            "\u001b[1m188/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 1.0701e-05Epoch 4: {'accuracy': 1.0, 'loss': 9.82369965640828e-06, 'val_accuracy': 1.0, 'val_loss': 8.252598490798846e-06}\n",
            "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 1.0692e-05 - val_accuracy: 1.0000 - val_loss: 8.2526e-06\n",
            "Epoch 5/20\n",
            "\u001b[1m188/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 7.6636e-06Epoch 5: {'accuracy': 1.0, 'loss': 7.166634986788267e-06, 'val_accuracy': 1.0, 'val_loss': 6.2601470744994e-06}\n",
            "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 7.6583e-06 - val_accuracy: 1.0000 - val_loss: 6.2601e-06\n",
            "Epoch 6/20\n",
            "\u001b[1m188/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 5.8990e-06Epoch 6: {'accuracy': 1.0, 'loss': 5.576401690632338e-06, 'val_accuracy': 1.0, 'val_loss': 4.985568466508994e-06}\n",
            "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 5.8956e-06 - val_accuracy: 1.0000 - val_loss: 4.9856e-06\n",
            "Epoch 7/20\n",
            "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 4.7374e-06Epoch 7: {'accuracy': 1.0, 'loss': 4.50619245384587e-06, 'val_accuracy': 1.0, 'val_loss': 4.080002327100374e-06}\n",
            "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 4.7362e-06 - val_accuracy: 1.0000 - val_loss: 4.0800e-06\n",
            "Epoch 8/20\n",
            "\u001b[1m186/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 3.8947e-06Epoch 8: {'accuracy': 1.0, 'loss': 3.720418817465543e-06, 'val_accuracy': 1.0, 'val_loss': 3.4021929877781076e-06}\n",
            "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 3.8910e-06 - val_accuracy: 1.0000 - val_loss: 3.4022e-06\n",
            "Epoch 9/20\n",
            "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 3.2661e-06Epoch 9: {'accuracy': 1.0, 'loss': 3.139583213851438e-06, 'val_accuracy': 1.0, 'val_loss': 2.910245939347078e-06}\n",
            "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 3.2654e-06 - val_accuracy: 1.0000 - val_loss: 2.9102e-06\n",
            "Epoch 10/20\n",
            "\u001b[1m188/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 2.8141e-06Epoch 10: {'accuracy': 1.0, 'loss': 2.717505822147359e-06, 'val_accuracy': 1.0, 'val_loss': 2.5466054012213135e-06}\n",
            "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 2.8131e-06 - val_accuracy: 1.0000 - val_loss: 2.5466e-06\n",
            "Epoch 11/20\n",
            "\u001b[1m188/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 2.4714e-06Epoch 11: {'accuracy': 1.0, 'loss': 2.3967493234522408e-06, 'val_accuracy': 1.0, 'val_loss': 2.262212547066156e-06}\n",
            "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 2.4706e-06 - val_accuracy: 1.0000 - val_loss: 2.2622e-06\n",
            "Epoch 12/20\n",
            "\u001b[1m185/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 2.1981e-06Epoch 12: {'accuracy': 1.0, 'loss': 2.1409318833320867e-06, 'val_accuracy': 1.0, 'val_loss': 2.031552185144392e-06}\n",
            "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 2.1966e-06 - val_accuracy: 1.0000 - val_loss: 2.0316e-06\n",
            "Epoch 13/20\n",
            "\u001b[1m186/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 1.9789e-06Epoch 13: {'accuracy': 1.0, 'loss': 1.9308351966174087e-06, 'val_accuracy': 1.0, 'val_loss': 1.8394546259514755e-06}\n",
            "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 1.9779e-06 - val_accuracy: 1.0000 - val_loss: 1.8395e-06\n",
            "Epoch 14/20\n",
            "\u001b[1m185/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 1.7947e-06Epoch 14: {'accuracy': 1.0, 'loss': 1.7543471813041833e-06, 'val_accuracy': 1.0, 'val_loss': 1.6766086901043309e-06}\n",
            "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 1.7937e-06 - val_accuracy: 1.0000 - val_loss: 1.6766e-06\n",
            "Epoch 15/20\n",
            "\u001b[1m187/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 1.6381e-06Epoch 15: {'accuracy': 1.0, 'loss': 1.603189389243198e-06, 'val_accuracy': 1.0, 'val_loss': 1.5358874634330277e-06}\n",
            "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 1.6375e-06 - val_accuracy: 1.0000 - val_loss: 1.5359e-06\n",
            "Epoch 16/20\n",
            "\u001b[1m184/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 1.5031e-06Epoch 16: {'accuracy': 1.0, 'loss': 1.4715636780238128e-06, 'val_accuracy': 1.0, 'val_loss': 1.41235034334386e-06}\n",
            "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 1.5021e-06 - val_accuracy: 1.0000 - val_loss: 1.4124e-06\n",
            "Epoch 17/20\n",
            "\u001b[1m186/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 1.3849e-06Epoch 17: {'accuracy': 1.0, 'loss': 1.355384824819339e-06, 'val_accuracy': 1.0, 'val_loss': 1.3027184877500986e-06}\n",
            "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 1.3843e-06 - val_accuracy: 1.0000 - val_loss: 1.3027e-06\n",
            "Epoch 18/20\n",
            "\u001b[1m183/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 1.2746e-06Epoch 18: {'accuracy': 1.0, 'loss': 1.2517522236521472e-06, 'val_accuracy': 1.0, 'val_loss': 1.2044696404700517e-06}\n",
            "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 1.2738e-06 - val_accuracy: 1.0000 - val_loss: 1.2045e-06\n",
            "Epoch 19/20\n",
            "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 1.1800e-06Epoch 19: {'accuracy': 1.0, 'loss': 1.1585073025344172e-06, 'val_accuracy': 1.0, 'val_loss': 1.1156874961670837e-06}\n",
            "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 1.1799e-06 - val_accuracy: 1.0000 - val_loss: 1.1157e-06\n",
            "Epoch 20/20\n",
            "\u001b[1m185/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 1.0932e-06Epoch 20: {'accuracy': 1.0, 'loss': 1.0739792060121545e-06, 'val_accuracy': 1.0, 'val_loss': 1.0349938293074956e-06}\n",
            "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 1.0927e-06 - val_accuracy: 1.0000 - val_loss: 1.0350e-06\n",
            "    epoch  accuracy      loss  val_accuracy  val_loss\n",
            "0       1  0.984899  0.040466           1.0  0.000039\n",
            "1       2  1.000000  0.000027           1.0  0.000019\n",
            "2       3  1.000000  0.000015           1.0  0.000012\n",
            "3       4  1.000000  0.000010           1.0  0.000008\n",
            "4       5  1.000000  0.000007           1.0  0.000006\n",
            "5       6  1.000000  0.000006           1.0  0.000005\n",
            "6       7  1.000000  0.000005           1.0  0.000004\n",
            "7       8  1.000000  0.000004           1.0  0.000003\n",
            "8       9  1.000000  0.000003           1.0  0.000003\n",
            "9      10  1.000000  0.000003           1.0  0.000003\n",
            "10     11  1.000000  0.000002           1.0  0.000002\n",
            "11     12  1.000000  0.000002           1.0  0.000002\n",
            "12     13  1.000000  0.000002           1.0  0.000002\n",
            "13     14  1.000000  0.000002           1.0  0.000002\n",
            "14     15  1.000000  0.000002           1.0  0.000002\n",
            "15     16  1.000000  0.000001           1.0  0.000001\n",
            "16     17  1.000000  0.000001           1.0  0.000001\n",
            "17     18  1.000000  0.000001           1.0  0.000001\n",
            "18     19  1.000000  0.000001           1.0  0.000001\n",
            "19     20  1.000000  0.000001           1.0  0.000001\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# # train the model\n",
        "# model.fit(x_train, y_train, epochs=20, batch_size=32, validation_data=(x_test, y_test))\n",
        "\n",
        "\n",
        "# Custom callback to save and print metrics at each epoch\n",
        "class EpochLogger(Callback):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.epochs = []\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        logs = logs or {}\n",
        "        # Print the metrics for the current epoch\n",
        "        print(f\"Epoch {epoch + 1}: {logs}\")\n",
        "\n",
        "        # Save metrics in a list for later use\n",
        "        self.epochs.append({\"epoch\": epoch + 1, **logs})\n",
        "\n",
        "# Instantiate the custom callback\n",
        "epoch_logger = EpochLogger()\n",
        "\n",
        "# Train the model and use the custom callback\n",
        "history = model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    epochs=20,\n",
        "    batch_size=32,\n",
        "    validation_data=(x_test, y_test),\n",
        "    callbacks=[epoch_logger]\n",
        ")\n",
        "\n",
        "# Save the epoch logs to a CSV file\n",
        "epoch_logs_df = pd.DataFrame(epoch_logger.epochs)\n",
        "epoch_logs_df.to_csv('epoch_logs.csv', index=False)\n",
        "\n",
        "# Print the saved dataframe\n",
        "print(epoch_logs_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSIayURaOMY2"
      },
      "source": [
        "### Test model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-5ejmH0OMY3",
        "outputId": "aa6ebb2a-066c-41ee-adca-a0ede64394af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
            "Predicted significant changes: (Binary) [0 0 0 0 0 0 0 0 0 0]\n",
            "Actual significant changes: (Binary) [0 0 0 0 0 0 0 0 0 0]\n"
          ]
        }
      ],
      "source": [
        "# Predict on the scaled test data\n",
        "y_pred_scaled = model.predict(x_test)\n",
        "\n",
        "# Apply inverse transformation to scale back to the original range and inverse transform y_test\n",
        "y_pred_binary = (y_pred_scaled > 0.5).astype(int)\n",
        "\n",
        "print(\"Predicted significant changes: (Binary)\", y_pred_binary.ravel()[:10])\n",
        "print(\"Actual significant changes: (Binary)\", y_test.ravel()[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHdxiFaJOMY3"
      },
      "source": [
        "## Other resources code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efmz2X9gOMY3"
      },
      "source": [
        "### Load model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YEMfTrU0OMY3"
      },
      "outputs": [],
      "source": [
        "loaded_model = load_model(Save_model_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9q0vB6QuOMY3"
      },
      "source": [
        "### Save model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "3g6C5f8kOMY4"
      },
      "outputs": [],
      "source": [
        "# save in tensoflow model\n",
        "Save_model_dir = 'LSTM_model.keras' # Add the .keras extension to the filename\n",
        "model.save(Save_model_dir)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
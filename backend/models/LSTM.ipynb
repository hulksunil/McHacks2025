{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Path to the ZIP file\n",
        "zip_file_path = '/content/TrainingData.zip'\n",
        "\n",
        "# Destination folder where files will be extracted\n",
        "extract_to_folder = '/content/'\n",
        "\n",
        "# Ensure the destination folder exists\n",
        "os.makedirs(extract_to_folder, exist_ok=True)\n",
        "\n",
        "# Open the ZIP file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    # Extract all the contents to the destination folder\n",
        "    zip_ref.extractall(extract_to_folder)\n",
        "\n",
        "print(f\"Files extracted to '{extract_to_folder}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRoz3CA7jNzF",
        "outputId": "8f9ea77a-ae9d-442c-ed0d-0c2ae93e456d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files extracted to '/content/'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xWrviVqQjqWI",
        "outputId": "528a8f16-52ed-4ad2-b236-d97feab98d0b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJW4DJj7OMYy"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "ucfgs04lOMYz"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "\n",
        "# Import necessary libraries for data processing\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Import necessary libraries for model\n",
        "from keras.callbacks import Callback\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import LSTM, Dense\n",
        "from keras.metrics import Precision, Recall, F1Score, AUC\n",
        "\n",
        "# Import necessary libraries for evaluation\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNvxW0SbOZyq"
      },
      "source": [
        "## Directories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "pLjNDLO0OgEd"
      },
      "outputs": [],
      "source": [
        "base_dir = '/content/drive/MyDrive/TrainingData'\n",
        "Save_model_dir = 'LSTM_model'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJ0EbNkkOMY0"
      },
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blmYvhkoOMY0"
      },
      "source": [
        "### Load and merge data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ptCZnn4OMY0",
        "outputId": "c7bfbf43-f4ed-4969-830c-89ce78c910e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File /content/drive/MyDrive/TrainingData/Period1/A/market_data_A_2.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/A/market_data_A_3.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/A/market_data_A_4.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/A/market_data_A_5.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/A/market_data_A_6.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/A/market_data_A_7.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/A/market_data_A_8.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/A/market_data_A_9.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/A/market_data_A_10.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/A/market_data_A_11.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/A/market_data_A_12.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/A/market_data_A_13.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/A/market_data_A_14.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/A/market_data_A_15.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/B/market_data_B_1.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/B/market_data_B_2.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/B/market_data_B_3.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/B/market_data_B_4.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/B/market_data_B_5.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/B/market_data_B_6.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/B/market_data_B_7.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/B/market_data_B_8.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/B/market_data_B_9.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/B/market_data_B_10.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/B/market_data_B_11.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/B/market_data_B_12.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/B/market_data_B_13.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/B/market_data_B_14.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/B/market_data_B_15.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/C/market_data_C_5.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/C/market_data_C_6.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/C/market_data_C_7.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/C/market_data_C_8.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/C/market_data_C_9.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/C/market_data_C_10.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/C/market_data_C_11.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/C/market_data_C_12.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/C/market_data_C_13.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/C/market_data_C_14.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/C/market_data_C_15.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/D/market_data_D_2.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/D/market_data_D_3.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/D/market_data_D_4.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/D/market_data_D_5.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/D/market_data_D_6.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/D/market_data_D_7.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/D/market_data_D_8.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/D/market_data_D_9.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/D/market_data_D_10.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/D/market_data_D_11.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/D/market_data_D_12.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/D/market_data_D_13.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/D/market_data_D_14.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/D/market_data_D_15.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/E/market_data_E_7.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/E/market_data_E_8.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/E/market_data_E_9.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/E/market_data_E_10.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/E/market_data_E_11.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/E/market_data_E_12.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/E/market_data_E_13.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/E/market_data_E_14.csv does not exist\n",
            "File /content/drive/MyDrive/TrainingData/Period1/E/market_data_E_15.csv does not exist\n"
          ]
        }
      ],
      "source": [
        "# List of stocks and splits\n",
        "periods = ['1']\n",
        "stocks = ['A', 'B', 'C', 'D', 'E']\n",
        "splits = ['0', '1', '2', '3', '4','5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15']\n",
        "\n",
        "# Load the data from a CSV file\n",
        "market_data_list = []\n",
        "trade_data_list = []\n",
        "\n",
        "# loop over the stocks and load the data for each period\n",
        "for period in periods:\n",
        "\n",
        "  # check for nested period dir\n",
        "  period_path = os.path.join(base_dir, f'Period{period}')\n",
        "  nested_period_path = os.path.join(period_path, f'Period{period}')\n",
        "\n",
        "  # Handle nested data\n",
        "  if os.path.exists(nested_period_path):\n",
        "      period_dir_to_use = nested_period_path\n",
        "  else:\n",
        "      period_dir_to_use = period_path\n",
        "\n",
        "\n",
        "  for stock in stocks:\n",
        "\n",
        "      trade_data_dir = f'{period_dir_to_use}/{stock}/trade_data__{stock}.csv'\n",
        "      if os.path.exists(trade_data_dir):\n",
        "          trade_data = pd.read_csv(trade_data_dir)\n",
        "          trade_data['stock'] = stock\n",
        "          trade_data_list.append(trade_data)\n",
        "      else:\n",
        "          print(f\"File {trade_data_dir} does not exist\")\n",
        "          continue\n",
        "\n",
        "      for split in splits:\n",
        "          market_data_dir = f'{period_dir_to_use}/{stock}/market_data_{stock}_{split}.csv'\n",
        "\n",
        "          if os.path.exists(market_data_dir):\n",
        "              market_data = pd.read_csv(market_data_dir)\n",
        "\n",
        "              # Add stock identifier\n",
        "              market_data['stock'] = stock\n",
        "              market_data_list.append(market_data)\n",
        "          else:\n",
        "              print(f\"File {market_data_dir} does not exist\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Columns in market_data:\", market_data.columns)\n",
        "print(\"Columns in trade_data:\", trade_data.columns)\n",
        "\n",
        "# Convert timestamps to datetime (if they aren't already)\n",
        "market_data['timestamp'] = pd.to_datetime(market_data['timestamp'])\n",
        "trade_data['timestamp'] = pd.to_datetime(trade_data['timestamp'])\n",
        "\n",
        "market_data = market_data.sort_values(by='timestamp')\n",
        "trade_data = trade_data.sort_values(by='timestamp')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OeNlEBL7HBAv",
        "outputId": "73242507-2728-4bc3-b8c8-193eb9af05bb"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns in market_data: Index(['bidVolume', 'bidPrice', 'askVolume', 'askPrice', 'timestamp', 'stock'], dtype='object')\n",
            "Columns in trade_data: Index(['price', 'volume', 'timestamp', 'stock'], dtype='object')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-31-6222e879ba96>:5: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  market_data['timestamp'] = pd.to_datetime(market_data['timestamp'])\n",
            "<ipython-input-31-6222e879ba96>:6: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  trade_data['timestamp'] = pd.to_datetime(trade_data['timestamp'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "merged_data = pd.merge_asof(\n",
        "    market_data.sort_values(by=[\"stock\", \"timestamp\"]),\n",
        "    trade_data[['stock','timestamp','price','volume']].sort_values(by=[\"stock\", \"timestamp\"]),\n",
        "    on='timestamp',\n",
        "    by='stock',            # match on the same stock symbol\n",
        "    direction='nearest',\n",
        "    tolerance=pd.Timedelta(seconds=1)\n",
        ")"
      ],
      "metadata": {
        "id": "zg-IX-sPHN2C"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6lkTQjKOMY0"
      },
      "source": [
        "### Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "K0_LdDQah51j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d9a1084-0855-4e09-8ae0-67254edc90b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-33-e51a599c65eb>:5: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
            "  merged_data['price_change'] = merged_data['price'].pct_change()\n"
          ]
        }
      ],
      "source": [
        "# define significant change as a percentage change of more than 1%\n",
        "threshold = 0.01\n",
        "\n",
        "# Calculate the spread between the market and trade data\n",
        "merged_data['price_change'] = merged_data['price'].pct_change()\n",
        "\n",
        "# calculate and label significant changes\n",
        "merged_data['significant_change'] = (merged_data['price_change'].abs() > threshold).astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6BKXhtTGOMY0",
        "outputId": "5ffc2cb0-cc85-4734-ac53-63978c8408bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-34-3e3a40589b58>:7: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  merged_data.fillna(method='bfill', inplace=True)\n",
            "<ipython-input-34-3e3a40589b58>:8: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  merged_data.fillna(method='ffill', inplace=True)\n"
          ]
        }
      ],
      "source": [
        "# Add a moving average of the spread\n",
        "merged_data['spread'] = merged_data['askPrice'] - merged_data['bidPrice']\n",
        "merged_data['bidPrice_ma'] = merged_data.groupby('stock')['bidPrice'].rolling(window=5).mean().reset_index(0, drop=True)\n",
        "merged_data['askPrice_ma'] = merged_data.groupby('stock')['askPrice'].rolling(window=5).mean().reset_index(0, drop=True)\n",
        "\n",
        "# Fill any remaining NANS by backfilling\n",
        "merged_data.fillna(method='bfill', inplace=True)\n",
        "merged_data.fillna(method='ffill', inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajLBxHIuOMY1"
      },
      "source": [
        "### Define the feature set and target set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "xRqfoZDjOMY1"
      },
      "outputs": [],
      "source": [
        "# Define the feature and it's fitted scaller\n",
        "features = [\n",
        "    'bidVolume',\n",
        "    'bidPrice',\n",
        "    'askVolume',\n",
        "    'askPrice',\n",
        "    'spread',\n",
        "    'bidPrice_ma',\n",
        "    'askPrice_ma'\n",
        "    ]\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "merged_data[features] = scaler.fit_transform(merged_data[features])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdWioYnnzgwh"
      },
      "source": [
        "### Preparing input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "2sO25n-Gh51k"
      },
      "outputs": [],
      "source": [
        "# define the timesteps\n",
        "timesteps = 10\n",
        "\n",
        "# Prepare input (x) and output (y)\n",
        "x = []\n",
        "y = []\n",
        "\n",
        "for stock in stocks:\n",
        "    stock_data = merged_data[merged_data['stock'] == stock]\n",
        "    stocks_features = stock_data[features].values\n",
        "    stocks_target = stock_data[['significant_change']].values\n",
        "\n",
        "    for i in range(len(stock_data) - timesteps):\n",
        "        x.append(stocks_features[i:i + timesteps])\n",
        "        y.append(stocks_target[i + timesteps])\n",
        "\n",
        "x = np.array(x)\n",
        "y = np.array(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eFiRRfOOMY1"
      },
      "source": [
        "### Split data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "LgANT82iOMY1"
      },
      "outputs": [],
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLks3o1jOMY1"
      },
      "source": [
        "# Implementation of Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vfDb4qDOMY2"
      },
      "source": [
        "### Define Model class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWGBvb4zOMY2",
        "outputId": "0aac0d48-e827-4783-c920-ee30c53449d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        }
      ],
      "source": [
        "# reshape the input data\n",
        "input_shape = (x_train.shape[1], x_train.shape[2])\n",
        "\n",
        "# define the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(100, input_shape=input_shape, return_sequences=True))\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUCwJCG9OMY2"
      },
      "source": [
        "### Compile model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "JcCbAQQHOMY2"
      },
      "outputs": [],
      "source": [
        "# compile the model for training\n",
        "model.compile(\n",
        "    loss='binary_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQo_W946OMY2"
      },
      "source": [
        "### Train and validate model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "Nqs219boOMY2",
        "outputId": "e086a6b8-1713-4cc4-9f36-27898f7bcdb6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Arguments `target` and `output` must have the same shape. Received: target.shape=(None, 1), output.shape=(None, 10)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-1084cfa76a98>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Train the model and use the custom callback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m history = model.fit(\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/nn.py\u001b[0m in \u001b[0;36mbinary_crossentropy\u001b[0;34m(target, output, from_logits)\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0me1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0me1\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0me2\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0me1\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0me2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    702\u001b[0m                 \u001b[0;34m\"Arguments `target` and `output` must have the same shape. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m                 \u001b[0;34m\"Received: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Arguments `target` and `output` must have the same shape. Received: target.shape=(None, 1), output.shape=(None, 10)"
          ]
        }
      ],
      "source": [
        "\n",
        "# # train the model\n",
        "# model.fit(x_train, y_train, epochs=20, batch_size=32, validation_data=(x_test, y_test))\n",
        "\n",
        "\n",
        "# Custom callback to save and print metrics at each epoch\n",
        "class EpochLogger(Callback):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.epochs = []\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        logs = logs or {}\n",
        "        # Print the metrics for the current epoch\n",
        "        print(f\"Epoch {epoch + 1}: {logs}\")\n",
        "\n",
        "        # Save metrics in a list for later use\n",
        "        self.epochs.append({\"epoch\": epoch + 1, **logs})\n",
        "\n",
        "# Instantiate the custom callback\n",
        "epoch_logger = EpochLogger()\n",
        "\n",
        "# Train the model and use the custom callback\n",
        "history = model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    epochs=20,\n",
        "    batch_size=32,\n",
        "    validation_data=(x_test, y_test),\n",
        "    callbacks=[epoch_logger]\n",
        ")\n",
        "\n",
        "# Save the epoch logs to a CSV file\n",
        "epoch_logs_df = pd.DataFrame(epoch_logger.epochs)\n",
        "epoch_logs_df.to_csv('epoch_logs.csv', index=False)\n",
        "\n",
        "# Print the saved dataframe\n",
        "print(epoch_logs_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSIayURaOMY2"
      },
      "source": [
        "### Test model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "id": "4-5ejmH0OMY3",
        "outputId": "dc3bb42e-88e8-4571-f73f-d248bc482da9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Found array with dim 3. None expected <= 2.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-cfa9a1dde4a0>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Apply inverse transformation to scale back to the original range and inverse transform y_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0my_pred_original\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_scaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred_scaled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0my_test_original\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_scaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36minverse_transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0mxp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_namespace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 573\u001b[0;31m         X = check_array(\n\u001b[0m\u001b[1;32m    574\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m             \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1099\u001b[0m             )\n\u001b[1;32m   1100\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_nd\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1101\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   1102\u001b[0m                 \u001b[0;34m\"Found array with dim %d. %s expected <= 2.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1103\u001b[0m                 \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found array with dim 3. None expected <= 2."
          ]
        }
      ],
      "source": [
        "# Predict on the scaled test data\n",
        "y_pred_scaled = model.predict(x_test)\n",
        "\n",
        "# Apply inverse transformation to scale back to the original range and inverse transform y_test\n",
        "y_pred_binary = (y_pred_scaled > 0.5).astype(int)\n",
        "\n",
        "print(\"Predicted significant changes: (Binary)\", y_pred_binary.ravel()[:10])\n",
        "print(\"Actual significant changes: (Binary)\", y_test.ravel()[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHdxiFaJOMY3"
      },
      "source": [
        "## Other resources code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efmz2X9gOMY3"
      },
      "source": [
        "### Load model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YEMfTrU0OMY3"
      },
      "outputs": [],
      "source": [
        "loaded_model = load_model(Save_model_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9q0vB6QuOMY3"
      },
      "source": [
        "### Save model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3g6C5f8kOMY4"
      },
      "outputs": [],
      "source": [
        "# save in tensoflow model\n",
        "model.save(Save_model_dir)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}